{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('mgu-lab': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f7380a7a517a3126ea41455c7f9370e55a346de8ff204edab6f6fc84119f7120"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM, ConvLSTM2D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activety types dict:\n",
    "Activety_Types = {'dws':1,'jog':2,'sit':3,'std':4,'ups':5,'wlk':6}        \n",
    "listDict = list(Activety_Types.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('input/A_DeviceMotion_data/dws_1/sub_10.csv', index_col=0)\n",
    "\n",
    "# cols = [0, 1, 2, 3,4, 5, 6,7,8]\n",
    "# i = 1\n",
    "# groups=cols\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# # fit and transform the data\n",
    "# df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# values = df_scaled.values\n",
    "# # plot each column\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# for group in groups:\n",
    "# \tplt.subplot(len(cols), 1, i)\n",
    "# \tplt.plot(values[:, group])\n",
    "# \tplt.title(df.columns[group], y=0.75, loc='right')\n",
    "# \ti += 1\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('test/dws/m.csv', index_col=0)\n",
    "\n",
    "# cols = [0, 1, 2, 3,4, 5, 6,7,8]\n",
    "# i = 1\n",
    "# groups=cols\n",
    "# scaler = preprocessing.RobustScaler()\n",
    "# # fit and transform the data\n",
    "# df_scaled = pd.DataFrame(scaler.fit_transform(df))\n",
    "\n",
    "# values = df_scaled.values\n",
    "# # plot each column\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# for group in groups:\n",
    "# \tplt.subplot(len(cols), 1, i)\n",
    "# \tplt.plot(values[:, group])\n",
    "# \tplt.title(df.columns[group], y=0.75, loc='right')\n",
    "# \ti += 1\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data folders:\n",
    "folders = glob('input/A_DeviceMotion_data/*_*')\n",
    "folders = [s for s in folders if \"csv\" not in s]\n",
    "\n",
    "X = list()\n",
    "y = list()\n",
    "n_timesteps = 120\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Load All data:\n",
    "for j  in folders:\n",
    "    csv = glob(j + '/*' )\n",
    "    # Get activity type from folder name\n",
    "    activity = Activety_Types[j[26:29]]\n",
    "\n",
    "    for i in csv:\n",
    "        values = pd.read_csv(i, index_col=0).values\n",
    "        \n",
    "        # Split data into samples of equal number of time steps\n",
    "        split_positions = list(range(n_timesteps, len(values), n_timesteps))\n",
    "        values = np.vsplit(values, split_positions)\n",
    "\n",
    "        # Make sure all samples are of equal size\n",
    "        if (len(values[-1]) < n_timesteps):\n",
    "            values = values[:-1]\n",
    "\n",
    "        X.extend(values)\n",
    "        y.extend(np.full(len(values), activity))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob('test/*')\n",
    "\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Load test data:\n",
    "for j  in folders:\n",
    "    csv = glob(j + '/*' )\n",
    "    # Get activity type from folder name\n",
    "    activity = Activety_Types[j[5:8]]\n",
    "\n",
    "    for i in csv:\n",
    "        values = pd.read_csv(i, index_col=0).values\n",
    "        # df_scaled = pd.DataFrame(scaler.fit_transform(df))\n",
    "        # values = df_scaled.values\n",
    "        \n",
    "        # Split data into samples of equal number of time steps\n",
    "        split_positions = list(range(n_timesteps, len(values), n_timesteps))\n",
    "        values = np.vsplit(values, split_positions)\n",
    "\n",
    "        # Make sure all samples are of equal size\n",
    "        if (len(values[-1]) < n_timesteps):\n",
    "            values = values[:-1]\n",
    "\n",
    "        X.extend(values)\n",
    "        y.extend(np.full(len(values), activity))\n",
    "\n",
    "X_test_new = np.array(X)\n",
    "y_test_new = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "for i in range(X_train.shape[-1]):\n",
    "    scalers[i] = preprocessing.StandardScaler()\n",
    "    X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i]) \n",
    "\n",
    "for i in range(X_test.shape[-1]):\n",
    "    X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "for i in range(X_test_new.shape[-1]):\n",
    "    X_test_new[:, :, i] = scalers[i].transform(X_test_new[:, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zero-offset class values\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "y_test_new = y_test_new - 1\n",
    "# one hot encode y\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_test_new = to_categorical(y_test_new)\n",
    "\n",
    "# Shape of X: [samples, time steps, features]\n",
    "n_features, n_outputs = X_train.shape[2], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "n_steps, n_length = 4, 30\n",
    "X_train = X_train.reshape((X_train.shape[0], n_steps, 1, n_length, n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], n_steps, 1, n_length, n_features))\n",
    "X_test_new = X_test_new.reshape((X_test_new.shape[0], n_steps, 1, n_length, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27aba90d3c8>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 0, 15, 64\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(ConvLSTM2D(16, \n",
    "                                       kernel_size = (1, 3),\n",
    "                                       activation='relu',\n",
    "                                       input_shape=(n_steps, 1, n_length, n_features),\n",
    "                                       return_sequences = True)))\n",
    "model.add(Bidirectional(ConvLSTM2D(32, \n",
    "                                       kernel_size = (3, 3),\n",
    "                                       padding = 'same',\n",
    "                                       return_sequences = True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[[253   1   1   0   5   1]\n [  3 282   0   0   1   0]\n [  0   0 718   0   0   0]\n [  0   0   0 617   0   0]\n [ 20   1   0   0 295   5]\n [ 12   1   0   0   7 677]], shape=(6, 6), dtype=int32)\n0.98\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, batch_size, use_multiprocessing=True)\n",
    "y_pred_max = tf.argmax(y_pred, axis=1)\n",
    "y_test_max = tf.argmax(y_test, axis=1)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(y_test_max, y_pred_max, n_outputs)\n",
    "score = accuracy_score(y_test_max, y_pred_max)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[[17  0  0  0  2  3]\n [ 0  0  0  0  0  0]\n [ 2  0 15  0  0  0]\n [ 0  0  0  6  0  0]\n [ 4  0  0  0 13  2]\n [ 9  0  3  2 13 13]], shape=(6, 6), dtype=int32)\n0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "y_pred_new = model.predict(X_test_new, batch_size, use_multiprocessing=True)\n",
    "y_pred_new_max = tf.argmax(y_pred_new, axis=1)\n",
    "y_test_new_max = tf.argmax(y_test_new, axis=1)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(y_test_new_max, y_pred_new_max, n_outputs)\n",
    "score = accuracy_score(y_test_new_max, y_pred_new_max)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}